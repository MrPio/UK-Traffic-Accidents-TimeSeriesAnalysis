{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis\n",
    "In this analysis we aim to predict the `number of accidents` and the `severity of the accidents`.\n",
    "\n",
    "## Setting up the dataset\n",
    "It is our intention to create a \"found\" time series, that is a dataset which is properly cleaned and structured to work with state of the art time series analysis algorithms. The reason for this is that our original Kaggle dataset was not intended to be used for time series analysis and was therefore not organized as such.\n",
    "\n",
    "In the following, we'll look at the main problems with this dataset, before outlining the strategy we've developed to address them.\n",
    "\n",
    "### The problem of a messy dataset\n",
    "The reader should bear in mind that the dataset used is a *government dataset*. As such, due to politics, budgets and other exogenous considerations, missing and inaccurate values, incomplete rows and discontinuities should be expected.\n",
    "\n",
    "And that's not even the half of it. Given the nature of the dataset we should expect the *time discounting* phenomenom to hinder the accuracy of the recorded information. More specifically, as this is an accident-related dataset, the event is recorded after the intervention of a traffic officer and after a report has been written. This means that the timestamp of the registration is likely to be distant from the timestamp of the accident, and details outside the report may be misreported, due to the *time discounting* phenomenom.\n",
    "\n",
    "### The problem of daylight savings time\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/time_zone.png\" width=\"60%\"/>\n",
    "</p>\n",
    "\n",
    "As the dataset is based on accidents that took place in the United Kingdom, the problem of time zone mismatch does not arise. In fact, the whole country is in the UTC time zone, so not only do we not have to worry about multiple time zones based on latitude and longitude values, but also we have equivalence between UTC and the local time zone.\n",
    "\n",
    "However, there's still the problem of daylight savings time, which causes some instants to occur twice a year and others to not exist at all.\n",
    "\n",
    "<!-- \n",
    "- time discounting, huge dataset => downsample with aggregation, timestamp should prevent lookahead\n",
    "- daylight savings => needs investigation, however we don't care due to the aggregation\n",
    "- utc (photo), fortunatly\n",
    "- null values?\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME='Accident_Information.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('../dataset', DATASET_NAME), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pruned = (\n",
    "    df[['Accident_Severity', 'Date', 'Time']]\n",
    "    .assign(timestamp=pd.to_datetime(df['Date'] + ' ' + df['Time']))\n",
    "    .drop(columns=['Date', 'Time'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Original size = 66.38 MB', 'Pruned size = 3.90 MB')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Original size = {df.size/1024 **\n",
    "                   2:.2f} MB', f'Pruned size = {df_pruned.size/1024**2:.2f} MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 rows have a null value, so I remove them\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(df_pruned[df_pruned.isnull().any(axis=1)])\n",
    "         } rows have a null value, so I remove them')\n",
    "df_pruned = df_pruned.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "\n",
    "<!-- \n",
    "- plot of 12 TS 1 for each year, and then 1 for each month\n",
    "- stagionality is additive or multiplicative? Should plot the residuals from the mean\n",
    "- stationarity, Aug Dickey Fuller (and KPSS?)\n",
    "- BoxCox\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
